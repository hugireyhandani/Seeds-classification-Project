# -*- coding: utf-8 -*-
"""Homework12_Seeds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Whc6DKaNyVT7Z1ioxX99AW9PYt5kbm25
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np


df = pd.read_excel('seeds_dataset.xlsx')

X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Selecting two class labels for the assignment
# Assuming we need to select class labels 1 and 2 (adjust if needed)
selected_classes = [1, 2]
X = X[y.isin(selected_classes)]
y = y[y.isin(selected_classes)]

# Splitting the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Function to train and evaluate an SVM with a given kernel
def train_evaluate_svm(kernel):
    svm = SVC(kernel=kernel)
    svm.fit(X_train, y_train)
    y_pred = svm.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    return accuracy, conf_matrix

# Training and evaluating SVMs with different kernels
linear_accuracy, linear_conf_matrix = train_evaluate_svm('linear')
gaussian_accuracy, gaussian_conf_matrix = train_evaluate_svm('rbf')  # Gaussian kernel is 'rbf' in sklearn
poly_accuracy, poly_conf_matrix = train_evaluate_svm('poly')  # Polynomial (degree 3)


print("SVM with Linear Kernel:\n Accuracy: {:.2f}%\n Confusion Matrix:\n {}\n".format(linear_accuracy * 100, linear_conf_matrix))
print("SVM with Gaussian Kernel:\n Accuracy: {:.2f}%\n Confusion Matrix:\n {}\n".format(gaussian_accuracy * 100, gaussian_conf_matrix))
print("SVM with Polynomial Kernel:\n Accuracy: {:.2f}%\n Confusion Matrix:\n {}".format(poly_accuracy * 100, poly_conf_matrix))

from sklearn.neighbors import KNeighborsClassifier

# Function to train and evaluate the kNN classifier
def train_evaluate_knn(n_neighbors):
    # Initialize the kNN model
    knn_model = KNeighborsClassifier(n_neighbors=n_neighbors)

    # Train the model on the training data
    knn_model.fit(X_train, y_train)

    # Make predictions on the test data
    y_pred = knn_model.predict(X_test)

    # Calculate accuracy and confusion matrix
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)

    return accuracy, conf_matrix

# Evaluating kNN classifier with a specific number of neighbors (e.g., 5)
knn_accuracy, knn_conf_matrix = train_evaluate_knn(5)

# Printing the results for the kNN classifier
print("kNN Classifier:\n Accuracy: {:.2f}%\n Confusion Matrix:\n {}".format(knn_accuracy * 100, knn_conf_matrix))

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

distortions = []
K = range(1, 9)
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(X)
    distortions.append(kmeanModel.inertia_)

# Plot the elbow
plt.figure(figsize=(16,8))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

import numpy as np

best_k = 4
kmeans = KMeans(n_clusters=best_k)
kmeans.fit(X)

# Randomly choose two features
features = np.random.choice(range(X.shape[1]), size=2, replace=False)

# Plot the clusters
plt.scatter(X.iloc[:, features[0]], X.iloc[:, features[1]], c=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:, features[0]], kmeans.cluster_centers_[:, features[1]], s=300, c='red', label='Centroids')
plt.xlabel(f'Feature {features[0]}')
plt.ylabel(f'Feature {features[1]}')
plt.legend()
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from scipy.stats import mode

best_k = 3

# Run k-means clustering with the best number of clusters
kmeans = KMeans(n_clusters=best_k, random_state=42)
kmeans.fit(X)

# Predict the cluster for each instance in the data
clusters = kmeans.predict(X)

# Initialize a dictionary to hold the majority class for each cluster
cluster_labels = {}

# Assign a label to each cluster based on the majority class
for cluster in range(best_k):
    # Get the indices of rows in this cluster
    indices = [i for i, x in enumerate(clusters) if x == cluster]
    # Get the class labels for these instances
    labels = y[indices]
    # Find the mode (most common element) of the class labels for this cluster
    cluster_label = mode(labels).mode[0] if len(labels) > 0 else None
    cluster_labels[cluster] = cluster_label

# Create a DataFrame to store the centroid information and assigned labels
cluster_info = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)
cluster_info['Assigned Label'] = cluster_labels.values()

# Display the DataFrame with centroid coordinates and assigned labels
print(cluster_info)

X_complete = df.iloc[:, :-1].values
y_true = df.iloc[:, -1].values

# Define the multi_label_classifier function
def multi_label_classifier(x, centroids):
    # Calculate the Euclidean distance between the point and each centroid
    distances = np.linalg.norm(x - centroids, axis=1)
    # Return the index of the nearest centroid
    return np.argmin(distances)

centroids = kmeans.cluster_centers_

# Apply the classifier to each data point in X_complete
predicted_labels = np.apply_along_axis(multi_label_classifier, 1, X_complete, centroids)

# Compute the accuracy assuming y_true contains the true labels
accuracy = np.mean(predicted_labels == y_true)
print(f'Accuracy of the multi-label classifier: {accuracy:.2f}')

import numpy as np
from sklearn.metrics import confusion_matrix

selected_labels = [1, 2]

# Create a mask for the data points where the true label is 1 or 2
mask = np.isin(y, selected_labels)

# Apply the mask to the true labels and the predicted labels to get the subset we are interested in
y_true_subset = y[mask]
predicted_labels_subset = mapped_labels[mask]

# Filter out the predictions for the third class since we are only interested in the two classes used for SVM
binary_mask = predicted_labels_subset != 3  # Excludes any predictions of class 3
binary_y_true_subset = y_true_subset[binary_mask]
binary_predicted_labels_subset = predicted_labels_subset[binary_mask]

# Compute the confusion matrix for the binary classification
binary_confusion_matrix_subset = confusion_matrix(binary_y_true_subset, binary_predicted_labels_subset)

print(binary_confusion_matrix_subset)